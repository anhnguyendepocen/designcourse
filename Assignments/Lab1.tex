\documentclass[a4paper,12pt]{article}
\usepackage[top=0in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{mdwlist}

\title{Statistical Analysis in R}
\author{}
\date{}

\begin{document}

\maketitle

\section{Purpose}

The purpose of this activity is to provide you with an understanding of statistical inference and to both develop and apply that knowledge to the use of the R statistical programming environment.

\section{Overview}

This lab can be completed during class time and at-home. You should allocate time to complete the relevant portions of the lab in line with the scheduled topics for each week. The two quantitative problem sets in LT relate to, roughly, the first half and second half of the material covered in this lab.

\section{Your Task}

Using R as instructed, complete the following activities.

\subsection{Sampling}

\begin{enumerate*}
\item As we talked about in lecture, simple random sampling is the easiest design-based strategy for ensuring that your sample data are \textit{representative} of the population from which those observations are drawn. The first of today's activities reiterates this idea using R. Start by defining an R vector that is going to contain our ``population'' values. This vector will store numerical values between 0 and 20; you can imagine that they represent the number of years of formal education obtained my members of our population:

\begin{verbatim}
set.seed(1)      # this makes sure we all get the same answer
x <- sample(0:20, 1e7, TRUE, c(1,2,3,4,5,6,7,8,9,10,11,12,13,12,11,10,9,8,7,6,5))
\end{verbatim}

\item Use the \texttt{length()} to verify how many people there are in our population.

\item Use \texttt{mean()} to calculate the ``true'' population mean of this population. How many years of education, on average, does this population have?

\item Now, we will draw a small sample from this population using the \texttt{sample()} function. (Note: we used this above to generate some fake population data; now we will use in a different way.). Start by drawing a small sample of just ten observations:

\begin{verbatim}
s1 <- sample(x, 5, FALSE)
\end{verbatim}

\item Use \textbf{ggplot2} to draw a histogram of these data:

\begin{verbatim}
ggplot(, aes(x = s1)) + geom_histogram(bins = 21)
\end{verbatim}

\item What is the sample mean of this sample?

\item Calculate the element variance of the data: $Var(Y) = s_Y^2 = \sum_{i=1}^{n} \dfrac{(Y_i - \bar{Y})^2}{n-1}$. In R this is just \texttt{sum((mean(s1) - s1)\^2)/(length(s1)-1)}. Or, more simply, \texttt{var(s1)}. Calculate the element standard deviation (you can use \texttt{sd()}).

\item Now, recall the formula for the standard error of the mean: $SE_{\bar{y}} = \sqrt{\frac{s^2}{n}}$, where $s^2 = $ sample (element) variance, and	$n = $ sample size.\footnote{Note we are sampling a very small proportion of our population, so we ignore the finite population correction.} Calculate the standard error of the population mean for your sample data.

\item Recall the definition of the margin of error. For a typical margin of error, we simply double the standard error to create an interval within which we estimate the population mean to be. What is the margin of error for your sample mean? Is the population mean in your sample mean?

\item Now, repeat the above exercises, but draw a larger sample size of size 100 (call this vector \texttt{s2}). How large is the margin of error of this larger sample compared to that of the smaller sample from before? Why?

\item Recall that the standard error is meant to capture the idea that if we repeated our sampling process and calculated our statistic of interest (in this case, the mean) on each sample, the standard deviation of those estimates around the true mean would be equal to our standard error. To get a better grasp on this idea, we are going to simulate the process of drawing random samples from our population and then compare the standard deviation of our estimates from each sample to the standard errors we calculate above.

\item To do this, we are going to use the \texttt{replicate()} function. This function allows us to repeat a calculate multiple times and return the results in a convenient form. To understand how it works, try generating a single sum of two random numbers: \texttt{rnorm(1) + rnorm(1)}. Then, use \texttt{replicate()} to do this five times:

\begin{verbatim}
replicate(5, rnorm (1) + rnorm (1))
\end{verbatim}

\noindent Note how the result is simply a vector.

\item Now, we want to apply this function to the calculation of the sample mean, as above. To do so, we simply write:

\begin{verbatim}
# five samples of size n = 5
replicate(5, mean(sample(x, 5, FALSE)))

# 1,000 samples of size n = 5
dist5 <- replicate(1000, mean(sample(x, 5, FALSE)))

# 100 samples of size n = 100
dist100 <- replicate(100, mean(sample(x, 100, FALSE)))
\end{verbatim}

\noindent Note that these operations may take some time. When they are done, examine the results:

\begin{itemize}
\item What does the histogram look like: \texttt{ggplot(, aes(x = dist5)) + geom\_histogram(bins = 21)} ?
\item Are the sample means ``unbiased'' (meaning the mean of the sample means is close to the population mean): \texttt{mean(dist5)} \texttt{mean(dist100)}?
\item How does the standard deviation of the sample means correspond to the standard errors you calculated above: \texttt{sd(dist5)} and \texttt{sd(dist100)} ?
\end{itemize}

\item The margin of error is a form of ``interval estimation'' in which we express our uncertainty about the value of a parameter by stating the range of values that the parameter is expected to be in based upon our sample estimate. The interval that is equivalent to estimate +/- 2 times the standard error is also called a 95\% confidence interval (for reasons we will return to below). You can compare the confidence interval from your data (mean +/- 2 SE) to the distribution of estimated sample means (\texttt{quantile(dist100, c(0.025, 0.975))}) to see how closely that interval compares to the interval of estimated values from repeated sampling.

\item To ensure you are comfortable with these ideas, try repeating all of the above but for a different kind of variable. Rather than using an ordinal or interval measure (as above), try using a binary variable. You can create one using \texttt{rbinom()}:

\begin{verbatim}
# the `prob` argument controls the ratio of 1s and 0s
y <- rbinom(1000000, 1, prob = .5) 
\end{verbatim}


\end{enumerate*}

\subsection{Descriptive Statistics}

\begin{enumerate*}
\item For the second major part of this activity, we will look at some real data from the Quality of Government project. This dataset contains country-level data on a very large number of economic, social, health, and political indicators. We will import the data using the \texttt{import()} function from the ``rio'' package. You may need to install this package using: \texttt{install.packages()}. Once the data are loaded, we can examine the data themselves by just confirming that they are loaded correctly:

\begin{verbatim}
library("rio")
d <- import("http://www.qogdata.pol.gu.se/data/qog_std_cs_jan16.dta")
dim(d)
nrow(d)
ncol(d)
names(d)
str(d)
\end{verbatim}

\item To obtain some simple descriptive statistics about a few variables, we can use the \texttt{summary()} function:

\begin{verbatim}
summary(d$fh_polity2)  # Polity scores (a democracy measure)
summary(d$gle_cgdpc)   # GDP
summary(d$dpi_finter)  # executive term limits
summary(d$bti_cr)      # civil rights index
summary(d$bl_asy15f)   # female educational attainment
\end{verbatim}

\item Use ggplot2 to create a histogram of the distributions of these variables (see code above). You may want to play with the \texttt{bins} argument to control the look of the histograms.

\item Use the R functions \texttt{mean()}, \texttt{median()}, and \texttt{table()} to inspect the central tendency and distribution of these variables.

\item To assess the dispersion of each variable, use the functions we used above: \texttt{var()} and \texttt{sd()}.

\item If you're feeling ambitious, you can create some of your own (``user-defined'') functions to calculate the skew and kurtosis statistics described by Kellstedt and Whitten:

\begin{verbatim}
skew <-  function(x) {
  m3 <- mean((x-mean(x))^3)
  skew <- m3/(sd(x)^3)
  skew
}
skew(d$gle_cgdpc)

kurtosis <- function(x) {  
  m4 <- mean((x-mean(x))^4) 
  kurt <- m4/(sd(x)^4)-3  
  kurt
}
kurtosis(d$gle_cgdpc)
\end{verbatim}

\item Skew and kurtosis are, in essence, meant to compare against a distribution known as the ``normal distribution.'' It is ``normal'' for statistical reasons that have little to do with ``normality'' in the sense of common English. It is also called the ``Gaussian'' distribution, and that can sometimes be a less confusing (those less commonly used label). A normal distribution looks like the following graph:

\begin{verbatim}
curve(dnorm, from = -4, to = 4, col = "red", lwd = 2)
\end{verbatim}


If a variable follows the normal distribution, its histogram will follow a very specific bell shape. We can ``eyeball'' this, but a more formal way to compare is by drawing what is called a ``Q-Q plot''. This is a special scatterplot drawn against a theoretical normal distribution based on the ``quantiles'' of the data (see \texttt{quantile()}). If the scatterplot has a straight line, then the data are normally distributed. If it deviates from that, then the data are skewed or ``peaked'' in a way that deviates from ``normality''. You can try it on two of the QoG variables:

\begin{verbatim}
# on two of our observed variables from QoG:
ggplot(d, aes(sample = gle_cgdpc)) + geom_qq()
ggplot(d, aes(sample = bl_asy15f)) + geom_qq()

# on a vector of random numbers drawn to follow the normal curve:
ggplot(, aes(sample = rnorm(1e5))) + geom_qq()
\end{verbatim}

\item Now repeat all of the above for the variables mentioned, and possibly explore other variables in the dataset. A codebook is available here: \url{http://qog.pol.gu.se/data/datadownloads/qogstandarddata}

\item Now, estimate the correlation between two variables. To do this, use \texttt{cor()}:

\begin{verbatim}
cor(d$gle_cgdpc, d$bl_asy15f)
\end{verbatim}

\noindent We can also generate a ``correlation matrix'' showing the correlation between many variables, but this requires specifying the data in a slightly different way:

\begin{verbatim}
cor(d[, c("gle_cgdpc", "bl_asy15f", "fh_polity2")])
\end{verbatim}

\item Based on the correlations, imagine what the scatterplots might look like (keeping in mind what the correlation coefficient measures). If the data are categorical (rather than interval), you may want to use a cross-tabulation rather than correlation coefficient to summarize the results:

\begin{verbatim}
table(d$dpi_finter, d$bti_cr)
\end{verbatim}

\noindent Note: You can also use \texttt{ftable()} to produce a slightly different looking table. You might also want to consider summarizing this relationship visually using a boxplot:

\begin{verbatim}
ggplot(d) + aes(x = factor(dpi_finter), y = bti_cr) + geom_boxplot()
\end{verbatim}

\item Use ggplot to create a scatterplot of the relationship between two variables. Here's an example showing the relationship between GDP (x-axis) and average female educational attainment:

\begin{verbatim}
ggplot(d) + aes(x = gle_cgdpc, y = bl_asy15f) + geom_point()
\end{verbatim}

\item You will note that R prints a warning message when producing this plot. This relates to missing values in the dataset, where one or both of these variables are unobserved for a particular country. To see which countries we are missing data for, try the following:

\begin{verbatim}
d$cname[is.na(d$gle_cgdpc)]  # for GDP
d$cname[is.na(d$bl_asy15f)]  # for educational attainment
\end{verbatim}

\noindent You can also look at the data directly to see where these missing values are. You can use \texttt{table(is.na(\textit{var}))} to count how many missing values there are in the variable. What does the presence of these missing values do for our ability to analyze the data? to estimate the values of population parameters? to represent the population? to draw a causal inference?

\item You may want to adjust the axis scales using, for example:

\begin{verbatim}
ggplot(d) + aes(x = gle_cgdpc, y = bl_asy15f) + geom_point() + scale_x_log10()
\end{verbatim}

\item You can modify the appearance of the plot in many, many ways. A common way to do this is by adding \texttt{aes()} features (see \texttt{? aes}) or by changing the plot theme (see \texttt{? theme}). Experiment with different plots until you feel comfortable with the various options.

\item One useful feature of ggplot2 is the ability to create multiple ``panels'' or ``facets'' (visual designer Edward Tufte calls these ``small multiples''). To do this, you use the \texttt{facet\_wrap()} function and specify a ``formula'' including the variable you would like to split the data by. This example create subpanels for different regions of the world:

\begin{verbatim}
ggplot(d) + aes(x = gle_cgdpc, y = bl_asy15f) + geom_point() + facet_wrap(~ht_region)
\end{verbatim}

\item Pause for a moment to consider how each facet represents a subset of the dataset. In this way, each facet is a summary of the dataset for only a subset of the dataset. If we want to summarize data in this way without plotting, we might consider using the \texttt{aggregate()} function. For example to calculate the mean level of GDP by region, you can do:

\begin{verbatim}
aggregate(gle_cgdpc ~ ht_region, data = d, FUN = mean)
\end{verbatim}

\noindent Try this aggregate command using different variables and using a different value of the \texttt{FUN} argument (which takes the name of a function, such as \texttt{mean}, \texttt{sd}, etc. without the parentheses) until you feel comfortable with the process of generating data summaries.



\end{enumerate*}

\subsection{Statistical Significance}

\begin{enumerate*}

\item For the final activity, we will examine the idea of ``statistical significance.'' Statistical significance is a concept related to \textit{statistical} hypothesis testing. If you recall from much earlier in the course (MT Week 6), we discussed two different ``flavours'' of hypothesis testing --- one associated with Fisher and one associated with Neyman and Pearson. We will see how both kinds of hypothesis testing manifest and how current statistical practice is a blend of these two perspectives. That practice most closely approximates Fisher's ideas (the calculation of a $p$-value) but differs in other ways (the estimation of a ``confidence interval'').

To put this exercise in context, we will focus on a common research question: do two groups significantly differ from one another on a variable of interest? For example, we might compare the post-graduation salaries of students in two degree programmes, levels of military spending in two types of countries, etc. In doing so, we might start with the null hypothesis that the two groups do not differ from one another (i.e., the ``difference-in-means'' is zero). Our alternative hypothesis is that the two groups differ from one another. But it might be easy for two groups to differ, just by chance, so the notion of statistical significance becomes useful for informing us about whether any difference we see between the two groups is larger than what would be expected given the variation we observe in values of the variable being studied.

This makes statistical significance closely related to the idea of repeated sampling that we examined earlier. We consider a difference ``statistically significant'' when it differs more from our null expectation than the variation we would expected to observe across repeated samples were our null hypothesis true (in this case, that there were actually no difference between the groups).\footnote{If we were talking about a different statistic, the logic would be the same. For a mean (like the one we looked at earlier), this would mean we could have a null expectation that the mean level of education in a country is 12 years. If we collect a sample of data and find that the mean is different from 12, we would consider that mean to be statistically significantly different from our null expectation of 12 years if the sample mean was further from 12 (i.e., much larger or much smaller) that the variation in sample means we could to estimate from same-sized samples drawn from a population where the true mean was 12. For a proportion, we might have a null expectation that the proportion is 0.5. An observed proportion in our data would be considered statistically significantly different from 0.5 if the proportion were larger than the variation in estimated proportions we would see across multiple same-sized samples from a population where the parameter was 0.5.} That was probably a bit confusing, but the following exercises should clarify things.

\item 


% We often use "no effect" null hypotheses

% "No effect" null hypotheses test whether $\theta$ is different from zero


% To conduct the test, we calculate a *t*-statistic: $t_{\hat{\beta_1}} = \frac{\hat{\beta_1}}{SE_{\hat{\beta_1}}}$
% When *t* is large enough, the cumulative probability of *t*-ratios larger than that value is small
% - i.e., large positive or large negative *t*-ratios put us in the tails of the t-distribution

% We are not restricted to "no effect" null hypotheses
% We can test against any null value
% To do so we simply calculate `\(\frac{\hat{\beta_1} - \alpha}{SE_{\hat{\beta_1}}}\)`, where `\(\alpha\)` is our null hypothesis value
% One such hypothesis would be `\(H_0: \beta_1=1\)`, to test whether there is a one-to-one correspondence between `\(x\)` and `\(y\)`

% The p-value is the probability of seeing a t-ratio/t-statistic as large or larger than the one we observed in our data, given the null hypothesis


% A small p-value represents: the probability of a *t*-statistic as extreme as the one we observed, if the null hypothesis was true


The p-value is not:

  - The probability that a hypothesis is true or false

  - A reflection of our confidence or certainty about the result

  - The probability that the true slope is in any particular range of values

  - A statement about the importance or substantive size of the effect
  
  




\item We now come to the idea of a ``confidence interval,'' which is an interval estimate like the interval created by calculating a margin of error 9as in the first part of this lab activity. A confidence interval (or ``CI'') is simply a range, centered on our sample estimate that tells us about the likely location of the population parameter within a stated range of uncertainty.\footnote{Because it is just a transformation of the margin of error, it is based on the variability of the data, sampling procedures, and --- most importantly --- sample size.} To formalize this, a confidence interval tells us:

\begin{quote}
Were we to repeat our procedure of sampling, analyzing the sample to produce a sample estimate and standard error, and transforming those estimates into a confidence interval \textit{repeatedly} from the population, a fixed percentage of the resulting intervals would include the true population-level parameter.
\end{quote}
        
\noindent This does not say for sure whether the estimated confidence interval \textit{this time} actually includes that true population parameter.
        

% calculate p-value based on a simulated distribution


        

\begin{verbatim}
y <- rnorm(100000,0,1)

# possible `alpha` levels
alpha <- qnorm(0.025) # 95% confidence interval
alpha <- qnorm(0.05)  # 90% confidence interval
alpha <- qnorm(0.33)  # 67% confidence interval
alpha <- qnorm(0.25)  # 50% confidence interval

n <- 100  # number of samples to draw and estimate statistic on
ci <- data.frame(i = 1:n,
                 means = numeric(n),
                 se = numeric(n),
                 off = logical(n))
for (i in 1:n){                             # Take 100 samples from our distribution
  tmp <- sample(y, 100, replace=FALSE)      # Store samples of set set w/o replacement in "temp"
  ci$means[i] <- mean(tmp)                  # calculate and store mean
  ci$se[i] <- (sd(tmp)/sqrt(length(tmp)))   # calculate and store upper CI limit
}
ci$off <- ((ci$means-(alpha*ci$se)) > 0 & (ci$means+(alpha*ci$se)) > 0) | 
          ((ci$means-(alpha*ci$se)) < 0 & (ci$means+(alpha*ci$se)) < 0)

ggplot(ci, aes(x = i, y = means, colour = off)) + 
    geom_errorbar(aes(ymin = (means - alpha*se), 
                      ymax = (means + alpha*se)), width=.1) + 
    geom_point() + coord_flip()
\end{verbatim}

This exercise shows that if a particular parameter value is true (in this case, the population mean is zero), we can draw confidence intervals for that mean to try to estimate where the mean is located. Most of these intervals will ``cover'' the true population parameter value, but not all of them. The number that cover the true population parameter value depends on the width of the confidence interval we draw. If we draw a wider confidence interval (say 95\%), then 95\% of the confidence intervals drawn from samples of this size from the population will cover the true population parameter value. If we draw a narrower confidence interval (say 50\%), then only 50\% of the confidence intervals drawn from samples of this size from the population will cover the true population value. 

When we set the width of the confidence interval as $1-\alpha$, we are saying we will allow $\alpha$ proportion of our confidence (where we to sample an infinite number of times) to not include the true population parameter. If $\alpha = 0.05$, then we are drawing 95\% confidence intervals. Thus only 5\% of the intervals we draw from this sample are likely to ``miss'' the true population parameter. If we therefore find in our particular sample that the interval differs from our null expectation (e.g., the sample mean does not equal zero and the 95\% confidence interval based upon our sample data does not cover 0), then we would say the sample mean difference is statistically significantly different from zero. (The $p$-value in this case would be less than 0.05.) So this either indicates that the population parameter is truly not equal to zero or that our particular sample happens to have produced one of the 5\% of confidence intervals that are expected (given our sampling procedure, sample size, and $\alpha$ level) to not cover the true population parameter, simply due to chance. We cannot know with certainty which interval we have.

\item We can also see, in the above data, the equivalence of a confidence interval, a $t$-statistic, and a $p$-value. When our confidence interval does not overlap the null hypothesis value, we describe it as statistically significant. The $p$-value is generated from a ``test statistic'' which translates our statistic (in the above example, the sample mean). For a sample mean, the $t$ statistic is simply the sample mean divided by the standard error. In our data that would be:

\begin{verbatim}
ci$mean / ci$se
\end{verbatim}

\noindent When this test statistic exceeds a ``critical value,'' then the statistic is deemed statistically significantly different from the null hypothesis value. The critical value is simply based on a hypothetical distribution, in this case the $t$-distribution (which, in large samples, is identical to the normal or Gaussian distribution noted earlier). When we earlier selected a value of $\alpha$ we were in essence setting the critical value of our $t$-statistic. Recall for our 95\% confidence interval, we selected an $\alpha$ value of \texttt{qnorm(0.025)} (approximately 1.96). Thus, when our sample $t$-statistic exceeds this value, then we have a statistically significant result because our test statistic is very far from the null hypothesis value of 0. Our null hypothesis implies a distribution of test statistics that follows the $t$ distribution and, given that expected distribution of possible test statistics, a sample test statistic larger than 1.96\% would be quite rare (less than 5\% of test statistics observed for samples from a population of mean 0 would have test statistics that large or larger).

\item This $t$-statistic can then be translated into the $p$-value: \texttt{1-pnorm(1.96)}\footnote{Here we see a ``two-tailed'' hypothesis test. We are allowing the test statistic to differ from the null expectation in either direction (see \texttt{1-pnorm(1.96)} and \texttt{pnorm(-1.96)}).} We can calculate the p-values for the $t$-statistic from each of our samples as:

\begin{verbatim}
pnorm(ci$mean / ci$se)
\end{verbatim}

\noindent You should now clearly be able to see a correspondence between $t$-statistics, the confidence intervals, and the $p$-values for each of the samples. The $t$-statistics are large when the confidence interval does not overlap zero:

\begin{verbatim}
cbind.data.frame(ci$means/ci$se, ci$off)
\end{verbatim}

\noindent and the $p$-value for each sample is small in those same cases.

\item A major caveat in any discussion of \textit{statistical} significance is that we can never forget \textit{substantive} significance. Statistical significance tells us whether an estimate, a relationship, or an effect is large relative to a hypothetical distribution of test statistics corresponding to null expectation. This says nothing about whether that effect is large or important in substantive terms. If we have enough data (i.e., our sample is large enough), almost any test statistic will ``statistically significant'' but that does not mean that the estimated parameter is large or important. If we find that democracy and non-democracies differ by \$50 in per capita GDP that this difference is statistically different from zero, that is a statistically significant difference. Whether that difference is large or important depends upon the state of broader scientific understanding, the amount of dispersion in the data (is the difference large if measured in number of standard deviations), the size of other differences (do regions of the world vary more than one another on this variable), the research context, and our own judgment. \$50 may be a substantively small effect when talking about GDP but it may be a large effect when talking about the cost of tonight's dinner. This is something for you to consider.


\end{enumerate*}

\section{Submission Instructions}

You will not submit this assignment for marking or feedback. We will continue working interactively with R in the coming weeks and you will have time to discuss your progress.

\end{document}


