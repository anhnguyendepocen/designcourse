\documentclass[a4paper,12pt]{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{hyperref}

\title{Statistical Analysis in R}
\author{}
\date{}

\begin{document}

\vspace{-4em}

\maketitle

\vspace{-4em}

\section{Purpose}

The purpose of this activity is to provide you with an understanding of statistical inference and to both develop and apply that knowledge to the use of the R statistical programming environment.

\section{Overview}

This lab can be completed during class time and at-home. You should allocate time to complete the relevant portions of the lab in line with the scheduled topics for each week. The final quantitative problem set in LT relates to, roughly, the first half and second half of the material covered in this lab (this document containing the first half) and builds on our previous work with data summaries and visualizations in R.

\section{Your Task}

Using R as instructed, complete the following activities.

\subsection{Statistical Significance}

\begin{enumerate}

\item For this activity, we will examine the idea of ``statistical significance.'' Statistical significance is a concept related to \textit{statistical} hypothesis testing. If you recall from much earlier in the course (LT Week 3), we discussed two different ``flavours'' of hypothesis testing --- one associated with Fisher and one associated with Neyman and Pearson. We will see how both kinds of hypothesis testing manifest and how current statistical practice is a blend of these two perspectives. That practice most closely approximates Fisher's ideas (the calculation of a $p$-value) but differs in other ways (the estimation of a ``confidence interval'').

\item To develop an initial understanding of the idea of statistical hypothesis testing, we will think about the idea of ``outliers.'' Outliers are unusual values in a variable (or set of variables). For example, consider the following variable:

\begin{verbatim}
v <- c(1,1,1,1,2,2,2,2,2,3,3,3,3,10)
\end{verbatim}

\noindent It should be clear that the value 10 here is an outlier. If we draw a boxplot of this variable, it becomes even more clear:

\begin{verbatim}
library("ggplot2")
ggplot(, aes(x = 1, y = v)) + geom_boxplot() + scale_y_continuous(limits = c(0,12))
\end{verbatim}

\item If we expected all the values of \texttt{v} to be relatively similar, we could point out that the value 10 is more than 3 standard deviations away from the mean of the variable: \texttt{(10-mean(v))/sd(v)}. This idea that a value is an outlier relative to some assumed central tendency is the basic logic of statistical hypothesis testing.

\item In statistical hypothesis testing, however, we are instead interested to know how unusual a \textit{statistic} is relative to our baseline expectation of what the value of the statistic should be. Commonly we are interested in statistics that describe a sample of cases:

\begin{itemize}
\item Mean years of education
\item Proportion of university graduates
\item The male--female mean difference in annual income
\item The correlations between education, sex, and income
\item The causal effect of education on annual income
\end{itemize}

Each of these statistics can be calculated on a sample of cases in order to make descriptive (or causal) inferences about the population of cases as a whole. But, given what we know about sampling (from MT Week 10), these inferences
 Thus instead of looking at the values of each observation in a variable and looking for how unusual they are, we want to know how unlikely we are to see a given statistic in our sample of values. But how do we determine whether a statistic is unusual? Think for a few minutes about how we would decide



\item To figure that out, we have to describe --- in Fisher's language --- a ``null hypothesis.'' This is a value that we think the parameter might have in the population. Often, the null hypothesis is set to something that would be theoretically uninteresting: a proportion is 0.5, a difference between two means is 0, a correlation between two variables is 0, the mean of a variable is 0, etc. We can pick any value, but these theoretically uninteresting values are conventional. We are therefore trying to see whether the statistic in our sample data differs from this (hypothetical) null population parameter.

\item But how do we know if our statistic differs from the null hypothesis value? We are worried that a difference between our sample statistic and the null hypothesis value might be erroneous. For example, we have a small sample so our data appear to differ from the null hypothesis but if we collected more observations the sample statistic on this larger sample might be closer to our null hypothesis for the population parameter. Statistical hypothesis testing therefore asserts that we only consider a sample statistic to differ from a null hypothesis value when it is quite far from the null hypothesis value. But how far exactly is ``quite far''?

To decide that, we return to the idea of repeated sampling that we examined earlier. We consider a difference ``statistically significant'' when it differs more from our null expectation than the variation in sample statistics we would expected to observe across repeated samples were our null hypothesis true (in this case, that there were actually no difference between the groups). For a mean (like the one we looked at earlier), this would be stated as a null expectation that the mean level of education in a country is 12 years. If we collect a sample of data and find that the mean is different from 12, we would consider that mean to be statistically significantly different from our null expectation of 12 years if the sample mean was further from 12 (i.e., much larger or much smaller) that the variation in sample means we could to estimate from same-sized samples drawn from a population where the true mean was 12.\footnote{If we were talking about a different statistic, the logic would be the same. For a proportion, we might have a null expectation that the proportion is 0.5. An observed proportion in our data would be considered statistically significantly different from 0.5 if the proportion were larger than the variation in estimated proportions we would see across multiple same-sized samples from a population where the parameter was 0.5.} 

\item Return to our vector of education values, \texttt{x}, from earlier and to our set of sample means of size 100 from that population, \texttt{dist100}. What is the standard deviation of the sampling distribution \texttt{dist100} (i.e., what is the standard error of the sample mean)? In statistical hypothesis testing, we will say that an estimate is statistically significantly different from the null hypothesis value when the sample statistic is more than a certain number of of standard errors away from the null hypothesis value. But how far is far enough to be considered \textit{significant}? Take a look at how many observations in our \texttt{dist100} vector are more than 3 standard errors, more than 2 standard errors, and more than 1 standard error above or below the true mean of \texttt{x}:

\begin{verbatim}
dist100[dist100 > (mean(x) + 3*sd(dist100))]
dist100[dist100 > (mean(x) + 2*sd(dist100))]
dist100[dist100 > (mean(x) + 1*sd(dist100))]
dist100[dist100 < (mean(x) - 3*sd(dist100))]
dist100[dist100 < (mean(x) - 2*sd(dist100))]
dist100[dist100 < (mean(x) - 1*sd(dist100))]
\end{verbatim}

\noindent Many sample means are within 1 standard error of the mean, fewer of them are further than 1 standard error from the population mean, even fewer are further than 2 standard errors, and --- in this example --- none or almost none are further than 3 standard errors from the mean. Thus when we repeatedly sample from this population, it is unusual to see a sample statistic more than 2 standard errors from the population mean and even more unusual to see a statistic more than 3 standard errors from the population mean.

\item Now here is where a short moment of consideration is required before we proceed. In statistical hypothesis testing, we are declaring a null hypothesis value as a \textit{known} population and seeing how likely different sample estimates are when draw repeated random samples from that population. We declare a sample statistic ``statistically significant'' when the sample statistic is an outlier in that distribution because it would seem to suggest that the sample is drawn from a population with a different population parameter value one with a population parameter equal to the null hypothesis. In other words, the sample statistic is so unusual for a the population from which we --- under the null hypothesis --- believe that the data are drawn from, that we decide that the data are actually drawn from a population with a different population mean. That's the essence of statistical hypothesis testing.

But, there's a leap being made here: in practice we don't generally have population data in front of us. We don't actually know the population mean (or any population parameter), so we can't draw repeated random samples from the population to create a sampling distribution. We just have the data in front of us. So, if we set a null expectation of the population parameter value and find that our sample statistic differs from that considerably, we are inclined to ``reject the null hypothesis'' and believe the population parameter value is different from our null expectation. But, this judgment is based on seeing a sample statistic that is \textit{unusual}, not \textit{impossible}, under the null hypothesis. We might therefore reject the null hypothesis sometimes when the population parameter actually is equal to the null hypothesis value. To avoid doing this too often, we have to define an ``error rate'' or ``significance level'' that only allows us to make these kind of ``false positive'' judgments quite rarely.

\item We'll call this significance level $\alpha$ in order to explore different possible values and what consequences that has for the probability of obtaining a ``false positive.'' A commonly used value of $\alpha$ is 0.05. This means that we will only declare a sample statistic to be ``statistically significant'' when it is as far or farther from the null hypothesis value as 5\% of the possible sample statistics generated from random samples from a population with a parameter equal to the null hypothesis value. In essence, we're looking for an outlier statistic that is farther than the 2.5\% percentile or 97.5\% percentile of the sampling distribution. If we look at our sampling distribution, we can see how far a sample statistic would have to be in order for us declare it statistically significantly different from the population mean:

\begin{verbatim}
quantile(dist100, c(0.025, 0.975))
\end{verbatim}

\noindent Check your understanding by assessing why these particular quantiles are used when $\alpha = 0.05$. Those quantiles reflect a ``two-tailed'' hypothesis test in which we look to see whether a sample statistic is an outlier in either direction. What quantiles would we consider if we were doing a ``one-tailed'' hypothesis to see if the sample statistic was an outlier only on the upper end of the distribution? only on the lower end of the distribution?

\item Different values of $\alpha$ are possible. Small values mean we want a lower chance of a ``false positive'' judgment, at the expense of making many for ``false negatives'' (judgments where we say a sample statistic is consistent with the null hypothesis when in fact it is drawn from a different population). Try some different values. Why are these two rates (false positives and false negatives) trade-offs?

\item But recall, we rarely have access to the population mean and we rarely have the ability to repeatedly sample from a population in order to create a sampling distribution. We therefore rely on ``distributional'' assumption. Rather than generating a sampling distribution from repeatedly sampling, rely on a mathematical theorem --- the central limit theorem --- that shows that the sampling distribution of the mean is normally distributed. You can get a sense of this by repeating our repeated sampling exercise above by collecting more samples. The more samples we draw from the population, the more and more the shape of the sampling distribution will resemble the normal distribution's bell curve:

\begin{verbatim}
ggplot(, aes(x = replicate(100, mean(sample(x, 10, FALSE))))) +
    geom_histogram(bins = 21)
ggplot(, aes(x = replicate(500, mean(sample(x, 10, FALSE))))) +
    geom_histogram(bins = 21)
ggplot(, aes(x = replicate(5000, mean(sample(x, 10, FALSE))))) +
    geom_histogram(bins = 21)
\end{verbatim}

\noindent This property enables us to calculate how unusual a sample estimate is against a null hypothesis by simply calculating the probability of seeing different statistic values given the normal distribution (which has a well-defined formula). In R we can calculate these probabilities using the \texttt{pnorm()} function. If we are thinking about sample means, we can calculate the probability of different sample means against any particular null hypothesis parameter value. To do so, however, requires that we express the mean as a difference from the null hypothesis value and rescale to the scale of standard errors. In our example, we have to convert the sample mean to number of years different from the population mean and then rescale to units in number of standard errors: i.e., from \texttt{mean(s2)} to 

\begin{verbatim}
tstat <- (mean(s2) - mean(x)) / ( sd(s2)/sqrt(length(s2)) )}
\end{verbatim}

\noindent This value is called a $t$-statistic. If we had a different null hypothesis value (e.g., that the population mean was 3), we would use that instead of \texttt{mean(x)} in the above calculation.

\item To see how unusual this $t$-statistic is, we plug it into the \texttt{pnorm()} function: \texttt{pnrom(tstat)}. The output is called a $p$-value and can be understood as the probability of seeing a test statistic this far or farther from the null hypothesis value. Formally, it is ``the probability of a $t$-statistic as extreme as the one we observed, if the null hypothesis was true.'' When this p-value is smaller than $\alpha$ level, we judge the sample mean to be ``statistically significantly different from zero.'' When the $p$-value is larger than $\alpha$, we declare that the test statistic is not statistically significantly different from the null hypothesis value. However, it is important to remember when reading this that:

\begin{itemize}
\item The probability that a hypothesis is true or false
\item A reflection of our confidence or certainty about the result
\item The probability that the true mean is in any particular range of values
\item A statement about the importance or substantive size of the effect
\end{itemize}

\noindent Those are all common misconceptions of how to interpret a $p$-value.

\item To see how large a $t$-statistic has to be to cross different $\alpha$ levels, you can explore the \texttt{qnorm()} function, which takes a $p$-value as input and returns the corresponding $t$-statistic.\footnote{Note: We are being a bit lose here. Technically, \texttt{qnorm()} looks at the normal distribution, but in very large samples, the normal distribution and the $t$ distribution are identical.}

\begin{verbatim}
qnorm(0.025) # 5% significance level
qnorm(0.05)  # 10% significance level
qnorm(0.33)  # 33% significance level
qnorm(0.25)  # 50% significance level
\end{verbatim}

\item We now come to the idea of a ``confidence interval,'' which is an interval estimate like the interval created by calculating a margin of error as in the first part of this lab activity. A confidence interval (or ``CI'') is simply a range, centered on our sample estimate that tells us about the likely location of the population parameter within a stated range of uncertainty.\footnote{Because it is just a transformation of the margin of error, it is based on the variability of the data, sampling procedures, and --- most importantly --- sample size.} To formalize this, a confidence interval tells us:

\begin{quote}
Were we to repeat our procedure of sampling, analyzing the sample to produce a sample estimate and standard error, and transforming those estimates into a confidence interval \textit{repeatedly} from the population, a fixed percentage of the resulting intervals would include the true population-level parameter.
\end{quote}
        
\noindent This does not say for sure whether the estimated confidence interval \textit{this time} actually includes that true population parameter.

\item To get at the notion of the confidence interval, we are going back to our population data \texttt{x} and repeatedly drawing new samples. This time, however, we are going to store our results in a data.frame and we are going to save not only how far the sample mean deviates from the population mean, but we are also going to save the standard error calculated from each sample:

\begin{verbatim}
alpha <- qnorm(0.025) # 5% significance level; 95% confidence interval

n <- 100  # number of samples to draw and estimate statistic on
ci <- data.frame(i = 1:n,
                 means = numeric(n),
                 se = numeric(n),
                 off = logical(n))
for (i in 1:n){                             # Take 100 samples from our distribution
  tmp <- sample(x, 100, replace=FALSE)      # Store samples in `temp`
  ci$means[i] <- mean(tmp)-mean(x)          # calculate and store mean
  ci$se[i] <- (sd(tmp)/sqrt(length(tmp)))   # calculate and store upper CI limit
}
\end{verbatim}

\noindent You can explore this new object, \texttt{ci}, perhaps using \texttt{summary(ci)}.

\item What we have generated here is a data.frame of ``centered'' sample means: we are expressing how far our sample mean is from the population mean. This allows us to calculate a so-called ``confidence'' interval. A confidence interval is an interval --- recall how we have already encountered it in the form of a margin of error --- that attempts to provide information about the location of the true population value. The confidence interval is sized based upon the variability of our data, the size of the sample we draw, and $\alpha$. The choice of $\alpha$, as in the rest of statistical hypothesis testing, is important because it dictates our error rate. 

When we set the width of the confidence level as $1-\alpha$, we are saying we will allow $\alpha$ proportion of our confidence (where we to sample an infinite number of times) to not include the true population parameter. If $\alpha = 0.05$, then we are drawing 95\% confidence intervals. Thus only 5\% of the intervals we draw from this sample are likely to ``miss'' the true population parameter. If we therefore find in our particular sample that the interval differs from our null expectation (e.g., the sample mean does not equal zero and the 95\% confidence interval based upon our sample data does not cover 0), then we would say the sample mean difference is statistically significantly different from zero. (The $p$-value in this case would be less than 0.05.) So this either indicates that the population parameter is truly not equal to zero or that our particular sample happens to have produced one of the 5\% of confidence intervals that are expected (given our sampling procedure, sample size, and $\alpha$ level) to not cover the true population parameter, simply due to chance. We cannot know with certainty which interval we have.

How many of our confidence intervals do not cover the population mean? (Recall we have rescaled the mean to be a different from the true value.):

\begin{verbatim}
ci$off <- ((ci$means-(abs(alpha)*ci$se)) > 0 & (ci$means+(abs(alpha)*ci$se)) > 0) | 
          ((ci$means-(abs(alpha)*ci$se)) < 0 & (ci$means+(abs(alpha)*ci$se)) < 0)
table(ci$off)
\end{verbatim}
 
\item We can graph all of our confidence intervals to see which include the true mean and which do not (colored by the \texttt{off} variable we just generated):

\begin{verbatim}
ggplot(ci, aes(x = i, y = means, colour = off)) + 
    geom_errorbar(aes(ymin = (means - alpha*se), 
                      ymax = (means + alpha*se)), width=.1) + 
    geom_point() + coord_flip()
\end{verbatim}

\noindent This exercise shows that if a particular parameter value is true (in this case, the population mean is zero), we can draw confidence intervals for that mean to try to estimate where the mean is located. Most of these intervals will ``cover'' the true population parameter value, but not all of them. The number that cover the true population parameter value depends on the width of the confidence interval we draw. If we draw a wider confidence interval (say 95\%), then 95\% of the confidence intervals drawn from samples of this size from the population will cover the true population parameter value. If we draw a narrower confidence interval (say 50\%), then only 50\% of the confidence intervals drawn from samples of this size from the population will cover the true population value. 

\item Try to repeat the above sampling and graphing procedure but tweak the values of $\alpha$ and sample size. As $\alpha$ increases, fewer of the confidence intervals drawn from the population with cover the true population parameter. That means that we are more likely to make ``false positive'' judgments and to have incorrect beliefs about the location of the population parameter.

\item We can also see, in the above data, the equivalence of a confidence interval, a $t$-statistic, and a $p$-value. When our confidence interval does not overlap the null hypothesis value, we describe it as statistically significant. The $p$-value is generated from a ``test statistic'' which translates our statistic (in the above example, the sample mean). For a sample mean, the $t$ statistic is simply the sample mean divided by the standard error. In our data that would be:

\begin{verbatim}
ci$mean / ci$se
\end{verbatim}

\noindent When this test statistic exceeds the critical value, $\alpha$, then the statistic is deemed statistically significantly different from the null hypothesis value. The critical value is simply based on a hypothetical distribution, in this case the $t$-distribution (which, in large samples, is identical to the normal or Gaussian distribution noted earlier). When we earlier selected a value of $\alpha$ we were in essence setting the critical value of our $t$-statistic. Recall for our 95\% confidence interval, we selected an $\alpha$ value of \texttt{qnorm(0.025)} (approximately 1.96). Thus, when our sample $t$-statistic exceeds this value, then we have a statistically significant result because our test statistic is very far from the null hypothesis value of 0. Our null hypothesis implies a distribution of test statistics that follows the $t$ distribution and, given that expected distribution of possible test statistics, a sample test statistic larger than 1.96\% would be quite rare (less than 5\% of test statistics observed for samples from a population of mean 0 would have test statistics that large or larger).

\item This $t$-statistic can then be translated into the $p$-value: \texttt{1-pnorm(1.96)}\footnote{Here we see a ``two-tailed'' hypothesis test. We are allowing the test statistic to differ from the null expectation in either direction (see \texttt{1-pnorm(1.96)} and \texttt{pnorm(-1.96)}).} We can calculate the p-values for the $t$-statistic from each of our samples as:

\begin{verbatim}
pnorm(ci$mean / ci$se)
\end{verbatim}

\noindent You should now clearly be able to see a correspondence between $t$-statistics, the confidence intervals, and the $p$-values for each of the samples. The $t$-statistics are large when the confidence interval does not overlap zero:

\begin{verbatim}
cbind.data.frame(ci$means/ci$se, ci$off)
\end{verbatim}

\noindent and the $p$-value for each sample is small in those same cases.


\item Returning to the QoG data from earlier, apply what we've just learned to assess whether the mean years of female educational attainment differ from a null hypothesis value. You can do this quickly in R using the \texttt{t.test()} function, where \texttt{mu} is the value of your null hypothesis:

\begin{verbatim}
t.test(d$bl_asy15f, mu = 12)
\end{verbatim}

\noindent Try out different possible values of \texttt{mu}.

\item A further test we might be interested in is whether countries' average educational attainment for men differs from the educational for women. This is what is known as a two-sample $t$-test. Like exercise we just performed to generate confidence intervals, this is based on a null hypothesis about the mean-difference, typically that the difference between two group means is zero. Try it out:

\begin{verbatim}
t.test(x = d$bl_asy15f, y = d$bl_asy15m)
\end{verbatim}

\item Another further test we might be interested in is whether two subsets of cases differ from one another. To capture this, we can also use \texttt{t.test()}, this time with a formula notation. For example we might test whether the countries with high and low levels of democracy differ in female educational attainment:

\begin{verbatim}
dem_high <- factor(d$fh_polity2 > mean(d$fh_polity2, na.rm = TRUE))
t.test(d$bl_asy15f ~ dem_high)
\end{verbatim}

\item Try to interpret all of the above results.

\item A major caveat in any discussion of \textit{statistical} significance is that we can never forget \textit{substantive} significance. Statistical significance tells us whether an estimate, a relationship, or an effect is large relative to a hypothetical distribution of test statistics corresponding to null expectation. This says nothing about whether that effect is large or important in substantive terms.\footnote{If we have enough data (i.e., our sample is large enough), almost any test statistic will ``statistically significant'' but that does not mean that the estimated parameter is large or important.} If we find that democracy and non-democracies differ by \$50 in per capita GDP that this difference is statistically different from zero, that is a statistically significant difference. Whether that difference is large or important depends upon the state of broader scientific understanding, the amount of dispersion in the data (is the difference large if measured in number of standard deviations), the size of other differences (do regions of the world vary more than one another on this variable), the research context, and our own judgment. \$50 may be a substantively small effect when talking about GDP but it may be a large effect when talking about the cost of tonight's dinner. This is something for you to consider.


\end{enumerate}

\end{document}